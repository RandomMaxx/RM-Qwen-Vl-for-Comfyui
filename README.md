Qwen3-VL ComfyUI Node Suite (High Performance)
A unified, production-grade ComfyUI node suite for Qwen3-VL Vision-Language models.
This suite has been re-engineered for reliability, speed, and ease of use. It features a Unified Loader that automatically handles both standard Transformers (High Precision) and GGUF (Low VRAM/High Speed) models, along with powerful image management tools.
üöÄ Key Features
üõ°Ô∏è Unified Backend Support
‚¶Å	Auto-Detect: The loader automatically identifies if a model is GGUF or Transformers based on the file/folder selected.
‚¶Å	Transformers: Full support for 4-bit/8-bit bitsandbytes quantization and Flash Attention 2.
‚¶Å	GGUF: Native llama-cpp-python integration for running 8B+ models on consumer GPUs (6GB-8GB VRAM) or Apple Silicon.
üì¶ Dedicated Model Manager
‚¶Å	GUI-Based Management: Download, Check Status, or Remove models directly from ComfyUI using the Model Manager node.
‚¶Å	JSON Config: All model URLs and settings are defined in qwen_models.json for easy updates.
üñºÔ∏è Advanced Image Handling
‚¶Å	Directory Scanning: The Image Loader can scan entire local folders, filter by name, and cycle through images by index.
‚¶Å	History System: Remembers your recently used folders for quick access.
‚¶Å	Smart Rescaling: Dedicated Image Rescaler ensures optimal resolution matching for VLM inputs.
üõ†Ô∏è Production Ready
‚¶Å	Logging: Detailed operation logs saved to logs/qwen3_vl.log.
‚¶Å	CUDA Safety: Strict VRAM cleanup, garbage collection, and OOM protection.
‚¶Å	Prompt Engineering: Built-in "Thinking", "Describe", and "Analyze" presets.
üì• Installation
Method 1: Manual (Recommended)
1.	Navigate to your ComfyUI custom nodes directory:  
cd ComfyUI/custom_nodes/
2.	Clone this repository:  
git clone [https://github.com/yourusername/ComfyUI-Qwen3-VL.git](https://github.com/yourusername/ComfyUI-Qwen3-VL.git)  
cd ComfyUI-Qwen3-VL
3.	Install Dependencies:
‚¶Å	Windows (Portable):  
..\..\..\python_embeded\python.exe -m pip install -r requirements.txt
‚¶Å	Linux / Mac / Venv:  
pip install -r requirements.txt
[!NOTE]  
For GGUF support on NVIDIA GPUs, ensure llama-cpp-python is compiled with CUDA support. If pre-built wheels fail, refer to the llama-cpp-python documentation.
üìÇ Folder Structure
The nodes expect models in specific directories inside your ComfyUI models folder.  
Logs and Configs are stored inside the custom node folder.  
ComfyUI/  
‚îú‚îÄ‚îÄ models/  
‚îÇ   ‚îî‚îÄ‚îÄ VLM/                        <-- Main Model Directory  
‚îÇ       ‚îú‚îÄ‚îÄ Qwen/  
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ Qwen3-VL-8B-Inst/   <-- Standard HF Models (Folders)  
‚îÇ       ‚îú‚îÄ‚îÄ qwen3-vl-8b-q4_k_m.gguf <-- GGUF Models (Single Files)  
‚îÇ       ‚îî‚îÄ‚îÄ mmproj-model-f16.gguf   <-- GGUF Vision Projectors  
‚îÇ  
‚îî‚îÄ‚îÄ custom_nodes/  
‚îî‚îÄ‚îÄ ComfyUI-Qwen3-VL/  
‚îú‚îÄ‚îÄ qwen_models.json        <-- Model Definitions (URLs/Names)  
‚îú‚îÄ‚îÄ qwen_history.json       <-- User Folder History  
‚îî‚îÄ‚îÄ logs/  
‚îî‚îÄ‚îÄ qwen3_vl.log        <-- Debug & Error Logs
üß© Node Overview
1. Management & Loading
RM-Qwen3-VL Model Manager
‚¶Å	Role: Your control center for model files.
‚¶Å	Inputs:
‚¶Å	model_name: Select from a predefined list (starred items * are not yet installed).
‚¶Å	action: Download (fetches from HF), Check Status, or Remove (deletes local files).
‚¶Å	Note: Supports both GGUF (files) and Transformers (folders) downloading.
Qwen3-VL Loader (GGUF + HF)
‚¶Å	Role: The universal model loader.
‚¶Å	Inputs:
‚¶Å	model: Select any installed model. The node auto-detects the backend.
‚¶Å	quantization: For Transformers models (4bit/8bit/None).
‚¶Å	gpu_layers_gguf: For GGUF models (-1 = Offload All to GPU).
2. Image Tools
RM-Qwen3-VL Image Loader
‚¶Å	Role: A powerful alternative to the standard "Load Image" node.
‚¶Å	Features:
‚¶Å	Directory Scan: Point it to a folder (directory) to load images sequentially.
‚¶Å	Quick Select: Dropdown menu remembers your last 15 used paths.
‚¶Å	Filtering: Use filter to find specific filenames (e.g., "landscape").
‚¶Å	Indexing: Use index to cycle through images in a folder.
RM-Qwen3-VL Image Rescaler
‚¶Å	Role: Prepares images for the VLM.
‚¶Å	Features: Resizes images to standard resolutions (e.g., 1024x1024) while maintaining aspect ratio and ensuring dimensions are divisible by the model's patch size (default modulo 16/32).
3. Inference
RM-Qwen3-VL Run
‚¶Å	Role: The main brain. Generates text from images.
‚¶Å	Inputs:
‚¶Å	caption_type: Choose presets like "Descriptive", "Thinking" (for COT models), "Danbooru Tags", or "JSON".
‚¶Å	prompt_mode: Prepend your custom prompt or Overwrite the system prompt.
‚¶Å	system_prompt: Fully customizable system instructions.
‚¶Å	boolean flags: Toggles for specific tasks (e.g., "Read Text", "Analyze Lighting", "Describe Clothing").
‚ö†Ô∏è Common Issues & Troubleshooting
1. "RuntimeError: CUDA Out of memory"
‚¶Å	Transformers: Use the Loader to set quantization to 4bit.
‚¶Å	GGUF: Reduce n_ctx (context size) or use a smaller quantization file (e.g., Q4_K_S).
‚¶Å	General: Enable unload_when_done in the Run node.
2. "GGUF Model missing mmproj file"
‚¶Å	Cause: GGUF models require a separate "Vision Projector" file (usually named mmproj-....gguf).
‚¶Å	Fix: Use the Model Manager node to download the GGUF model; it automatically fetches the required mmproj file defined in qwen_models.json.
3. "No models found"
‚¶Å	Fix: Connect the Model Manager node, select a model, set action to Download, and run the queue once.
4. Logs & Debugging
‚¶Å	Check custom_nodes/ComfyUI-Qwen3-VL/logs/qwen3_vl.log for detailed error messages and download progress.
üìú License
This custom node is open-source. The Qwen3-VL models are subject to the original license by the Qwen Team (Alibaba Cloud).
