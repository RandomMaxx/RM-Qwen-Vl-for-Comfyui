{
  "huggingface_models": {
    "hf_vl_models": {
      "Qwen3-VL-2B-Instruct": {
        "repo_id": "Qwen/Qwen3-VL-2B-Instruct",
        "default": true,
        "quantized": false,
        "vram_requirement": {
          "full": 4.0,
          "8bit": 2.5,
          "4bit": 1.5
        }
      },
      "Qwen3-VL-2B-Thinking": {
        "repo_id": "Qwen/Qwen3-VL-2B-Thinking",
        "default": false,
        "quantized": false,
        "vram_requirement": {
          "full": 4.0,
          "8bit": 2.5,
          "4bit": 1.5
        }
      },
      "Qwen3-VL-2B-Instruct-FP8": {
        "repo_id": "Qwen/Qwen3-VL-2B-Instruct-FP8",
        "default": false,
        "quantized": true,
        "vram_requirement": {
          "full": 2.5
        }
      },
      "Qwen3-VL-2B-Thinking-FP8": {
        "repo_id": "Qwen/Qwen3-VL-2B-Thinking-FP8",
        "default": false,
        "quantized": true,
        "vram_requirement": {
          "full": 2.5
        }
      },
      "Qwen3-VL-4B-Instruct": {
        "repo_id": "Qwen/Qwen3-VL-4B-Instruct",
        "default": true,
        "quantized": false,
        "vram_requirement": {
          "full": 6.0,
          "8bit": 3.5,
          "4bit": 2.0
        }
      },
      "Qwen3-VL-4B-Thinking": {
        "repo_id": "Qwen/Qwen3-VL-4B-Thinking",
        "default": false,
        "quantized": false,
        "vram_requirement": {
          "full": 6.0,
          "8bit": 3.5,
          "4bit": 2.0
        }
      },
      "Qwen3-VL-4B-Instruct-FP8": {
        "repo_id": "Qwen/Qwen3-VL-4B-Instruct-FP8",
        "default": false,
        "quantized": true,
        "vram_requirement": {
          "full": 2.5
        }
      },
      "Qwen3-VL-4B-Thinking-FP8": {
        "repo_id": "Qwen/Qwen3-VL-4B-Thinking-FP8",
        "default": false,
        "quantized": true,
        "vram_requirement": {
          "full": 2.5
        }
      },
      "Qwen3-VL-8B-Instruct": {
        "repo_id": "Qwen/Qwen3-VL-8B-Instruct",
        "default": false,
        "quantized": false,
        "vram_requirement": {
          "full": 12.0,
          "8bit": 7.0,
          "4bit": 4.5
        }
      },
      "Qwen3-VL-8B-Thinking": {
        "repo_id": "Qwen/Qwen3-VL-8B-Thinking",
        "default": false,
        "quantized": false,
        "vram_requirement": {
          "full": 12.0,
          "8bit": 7.0,
          "4bit": 4.5
        }
      },
      "Qwen3-VL-8B-Instruct-FP8": {
        "repo_id": "Qwen/Qwen3-VL-8B-Instruct-FP8",
        "default": false,
        "quantized": true,
        "vram_requirement": {
          "full": 7.5
        }
      },
      "Qwen3-VL-8B-Thinking-FP8": {
        "repo_id": "Qwen/Qwen3-VL-8B-Thinking-FP8",
        "default": false,
        "quantized": true,
        "vram_requirement": {
          "full": 7.5
        }
      },
      "Qwen3-VL-32B-Instruct": {
        "repo_id": "Qwen/Qwen3-VL-32B-Instruct",
        "default": false,
        "quantized": false,
        "vram_requirement": {
          "full": 28.0,
          "8bit": 14.0,
          "4bit": 8.5
        }
      },
      "Qwen3-VL-32B-Thinking": {
        "repo_id": "Qwen/Qwen3-VL-32B-Thinking",
        "default": false,
        "quantized": false,
        "vram_requirement": {
          "full": 28.0,
          "8bit": 14.0,
          "4bit": 8.5
        }
      },
      "Qwen3-VL-32B-Instruct-FP8": {
        "repo_id": "Qwen/Qwen3-VL-32B-Instruct-FP8",
        "default": false,
        "quantized": true,
        "vram_requirement": {
          "full": 24.0
        }
      },
      "Qwen3-VL-32B-Thinking-FP8": {
        "repo_id": "Qwen/Qwen3-VL-32B-Thinking-FP8",
        "default": false,
        "quantized": true,
        "vram_requirement": {
          "full": 24.0
        }
      },
      "Qwen2.5-VL-3B-Instruct": {
        "repo_id": "Qwen/Qwen2.5-VL-3B-Instruct",
        "default": false,
        "quantized": false,
        "vram_requirement": {
          "full": 6.0,
          "8bit": 3.5,
          "4bit": 2.0
        }
      },
      "Qwen2.5-VL-7B-Instruct": {
        "repo_id": "Qwen/Qwen2.5-VL-7B-Instruct",
        "default": false,
        "quantized": false,
        "vram_requirement": {
          "full": 15.0,
          "8bit": 8.5,
          "4bit": 5.0
        }
      }
    },
    "hf_text_models": {
      "Qwen3-0.6B": {
        "repo_id": "Qwen/Qwen3-0.6B",
        "default": false,
        "quantized": false
      },
      "qwen3-4b-Z-Image-Engineer": {
        "repo_id": "BennyDaBall/qwen3-4b-Z-Image-Engineer",
        "default": false,
        "quantized": false
      },
      "Qwen3-4B-Instruct-2507": {
        "repo_id": "Qwen/Qwen3-4B-Instruct-2507",
        "default": false,
        "quantized": false
      }
    }
  },
  "GGUF_models": {
    "base_dir": "llm/GGUF",
    "Qwen_model": {
      "Qwen3-4B-GGUF": {
        "author": "Qwen",
        "repo_name": "Qwen3-4B-GGUF",
        "repo_id": "Qwen/Qwen3-4B-GGUF",
        "model_files": [
          "Qwen3-4B-Q4_K_M.gguf",
          "Qwen3-4B-Q5_0.gguf",
          "Qwen3-4B-Q5_K_M.gguf",
          "Qwen3-4B-Q6_K.gguf",
          "Qwen3-4B-Q8_0.gguf"
        ],
        "defaults": {
          "context_length": 8192
        }
      },
      "Qwen3-4B-abliterated-GGUF": {
        "author": "Mungert",
        "repo_name": "Qwen3-4B-abliterated-GGUF",
        "repo_id": "Mungert/Qwen3-4B-abliterated-GGUF",
        "alt_repo_ids": [
          "mradermacher/Qwen3-4B-abliterated-GGUF"
        ],
        "model_files": [
          "Qwen3-4B-abliterated-iq2_m.gguf",
          "Qwen3-4B-abliterated-iq2_s.gguf",
          "Qwen3-4B-abliterated-iq2_xs.gguf",
          "Qwen3-4B-abliterated-iq2_xxs.gguf",
          "Qwen3-4B-abliterated-iq3_m.gguf",
          "Qwen3-4B-abliterated-iq3_s.gguf",
          "Qwen3-4B-abliterated-iq3_xs.gguf",
          "Qwen3-4B-abliterated-iq3_xxs.gguf",
          "Qwen3-4B-abliterated-iq4_nl.gguf",
          "Qwen3-4B-abliterated-iq4_xs.gguf",
          "Qwen3-4B-abliterated-q2_k_s.gguf",
          "Qwen3-4B-abliterated-q3_k_m.gguf",
          "Qwen3-4B-abliterated-q3_k_s.gguf",
          "Qwen3-4B-abliterated-q4_0.gguf",
          "Qwen3-4B-abliterated-q4_1.gguf",
          "Qwen3-4B-abliterated-q4_k_m.gguf",
          "Qwen3-4B-abliterated-q4_k_s.gguf",
          "Qwen3-4B-abliterated-q5_0.gguf",
          "Qwen3-4B-abliterated-q5_1.gguf",
          "Qwen3-4B-abliterated-q5_k_m.gguf",
          "Qwen3-4B-abliterated-q5_k_s.gguf",
          "Qwen3-4B-abliterated-q6_k_m.gguf",
          "Qwen3-4B-abliterated-q8_0.gguf",
          "Qwen3-4B-abliterated-bf16.gguf",
          "Qwen3-4B-abliterated-bf16_q8_0.gguf",
          "Qwen3-4B-abliterated-f16_q8_0.gguf"
        ],
        "defaults": {
          "context_length": 8192
        }
      }
    },
    "qwenVL_model": {
      "Qwen3-VL-4B-Instruct-GGUF": {
        "author": "Qwen",
        "repo_name": "Qwen3-VL-4B-Instruct-GGUF",
        "repo_id": "Qwen/Qwen3-VL-4B-Instruct-GGUF",
        "mmproj_file": "mmproj-Qwen3VL-4B-Instruct-F16.gguf",
        "model_files": [
          "Qwen3VL-4B-Instruct-Q4_K_M.gguf",
          "Qwen3VL-4B-Instruct-Q8_0.gguf",
          "Qwen3VL-4B-Instruct-F16.gguf"
        ],
        "defaults": {
          "context_length": 8192,
          "image_max_tokens": 4096,
          "n_batch": 512,
          "gpu_layers": -1,
          "top_k": 0,
          "pool_size": 4194304
        }
      },
      "Qwen3-VL-8B-Instruct-GGUF": {
        "author": "Qwen",
        "repo_name": "Qwen3-VL-8B-Instruct-GGUF",
        "repo_id": "Qwen/Qwen3-VL-8B-Instruct-GGUF",
        "mmproj_file": "mmproj-Qwen3VL-8B-Instruct-F16.gguf",
        "model_files": [
          "Qwen3VL-8B-Instruct-Q4_K_M.gguf",
          "Qwen3VL-8B-Instruct-Q8_0.gguf",
          "Qwen3VL-8B-Instruct-F16.gguf"
        ],
        "defaults": {
          "context_length": 8192,
          "image_max_tokens": 4096,
          "n_batch": 512,
          "gpu_layers": -1,
          "top_k": 0,
          "pool_size": 4194304
        }
      },
      "Qwen3-VL-4B-Thinking-GGUF": {
        "author": "Qwen",
        "repo_name": "Qwen3-VL-4B-Thinking-GGUF",
        "repo_id": "Qwen/Qwen3-VL-4B-Thinking-GGUF",
        "mmproj_file": "mmproj-Qwen3VL-4B-Thinking-F16.gguf",
        "model_files": [
          "Qwen3VL-4B-Thinking-Q4_K_M.gguf",
          "Qwen3VL-4B-Thinking-Q8_0.gguf",
          "Qwen3VL-4B-Thinking-F16.gguf"
        ],
        "defaults": {
          "context_length": 8192,
          "image_max_tokens": 4096,
          "n_batch": 512,
          "gpu_layers": -1,
          "top_k": 0,
          "pool_size": 4194304
        }
      },
      "Qwen3-VL-8B-Thinking-GGUF": {
        "author": "Qwen",
        "repo_name": "Qwen3-VL-8B-Thinking-GGUF",
        "repo_id": "Qwen/Qwen3-VL-8B-Thinking-GGUF",
        "mmproj_file": "mmproj-Qwen3VL-8B-Thinking-F16.gguf",
        "model_files": [
          "Qwen3VL-8B-Thinking-Q4_K_M.gguf",
          "Qwen3VL-8B-Thinking-Q8_0.gguf",
          "Qwen3VL-8B-Thinking-F16.gguf"
        ],
        "defaults": {
          "context_length": 8192,
          "image_max_tokens": 4096,
          "n_batch": 512,
          "gpu_layers": -1,
          "top_k": 0,
          "pool_size": 4194304
        }
      }
    }
  }
}